#!/usr/bin/env python

## This code was modified from the aligner available at 
## https://github.com/callison-burch/dreamt.git

import optparse
import sys
import os
import time
from collections import defaultdict
import string

optparser = optparse.OptionParser()
## Options that give the aligner which files to work with
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--iterations", dest="iterations", default=1, type="int", help="Number of iterations for which to run ibm1 algorithm (default=1)")
optparser.add_option("-k", "--hmm_iterations", dest="hmm", default=1, type="int", help="Number of iterations for which to run hmm algorithm (default=1)")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

def clean_word(word):
  return word.lower()

def ibm1():
  # set up pairs of sentences, with NULL inserted in foreign language
  pairs = [[[None] + [clean_word(word) for word in pair[0].strip().split()], 
            [clean_word(word) for word in pair[1].strip().split()]] 
           for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
  ## setting up initial values
  t = defaultdict(int)
  total = defaultdict(int)
  for pair in pairs:
    for w1 in pair[1]:
      for w0 in pair[0]:
        total[w1] += 1
        t[(w1, w0)] += 1
  for (w1, w0) in t.keys():
    t[(w1, w0)] = float(t[(w1, w0)]) / total[w1]
  ## Loop through iterations
  for i in range(opts.iterations):
    sys.stderr.write("Now doing iteration %i for ibm1\n" % (i + 1))
    count = defaultdict(float)
    total = defaultdict(float)
    for pair in pairs:
      for w1 in pair[1]:
        denom = sum([t[(w1, w0)] for w0 in pair[0]])
        for w0 in pair[0]:
          count[(w1, w0)] += t[(w1, w0)] / denom
          total[w0] += t[(w1, w0)] / denom
    for (w1, w0) in t.keys(): 
      t[(w1, w0)] = count[(w1, w0)] / total[w0] 
  return (t, pairs)

# Run the IBM Model 1
(t, pairs) = ibm1()

# Append a path (alignment) to every pair of sentences
for pair in pairs:
  pair.append([0 for w0 in pair[1]])

max_f_len = max([len(pair[0]) for pair in pairs]) 
max_e_len = max([len(pair[1]) for pair in pairs])

# Instantiate all the necessary arguments
s = [0.2 for x in range(2*max_f_len - 1)]
pi = [0.1 for x in range(max_f_len)]
for i in range(1, max_f_len):
  s[i] = 1.0 / i
  s[-i] = 0.5 / i
  pi[i] = 1.0 / i
s = [si / sum(s) for si in s]
pi = [ppi / sum(pi) for ppi in pi]
p_null = 0.01

def update_params():
  pi_count = [0 for x in range(max_f_len)]        # initial probs
  s_count = [0 for x in range(2*max_f_len - 1)]   # transition probabilities
  null_count = 0                                  # English words not from foreign words
  for pair in pairs:
    path = pair[2]                                    # Current path, as determined by Viterbi algorithm
    if path[0] != 0: pi_count[path[0]] += 1           # initial prob count increased by first position in path; we never treat NULL as the start
    for i in range(1, len(path)):
      prev = i - 1                        
      if path[i] == 0: null_count += 1                         # if the English word came from NULL, increase null-count
      while path[prev] == 0 and prev > 0: prev -= 1            # if the previous word is NULL, look for the word before that; stop when you reach zero
      if path[prev] != 0: s_count[path[i] - path[prev]] += 1   # we have now jumped the distance from the last word to this one; don't count jumps from 0
      else: pi_count[path[i]] += 1                             # we never treat NULL as the start of a sentence
  # normalize params
  p_null = null_count / float(sum(s_count) + null_count)
  s = [si / float(sum(s_count)) for si in s_count]
  pi = [ppi / float(sum(pi_count)) for ppi in pi_count]
  return (p_null, s, pi)

def viterbi():
  for (pairnum, pair) in enumerate(pairs):
    if pairnum % 5000 == 0: sys.stderr.write("Done %i pairs...\n" % (pairnum))
    # length of sequence; equal to length of English sentence
    time = range(len(pair[1]))
    # number of possible states; equal to length of foreign sentence
    states = range(len(pair[0]))
    s_denom = [sum([s[j - k] for j in states]) for k in states]
    not_p_null = 1 - p_null
    delta = [[0.0 for st in states] for ti in time]
    gamma = [[0 for st in states] for ti in time]
    # delta 0 corresponds to the start state (time 0); probability of starting for every French word
    for st in states:
      ### Heuristic: if the word appears exactly in both sentences, it should not be translated from another word
      ####################################################
      if pair[1][0] in pair[0] and pair[0][st] != pair[1][0]: t_prob = 0.0
      else: t_prob = t[(pair[1][0], pair[0][st])]
      ####################################################
      delta[0][st] = t_prob * pi[st]
      delta[0][0] = p_null # Make a special provision for NULL at the beginning of a sentence
    # now we step through the rest of the time period using dynamic programming
    for ti in time[1:]:
      for curr_state in states:
        max_val = -1 # the maximum probability (of states from which this one could have come)
        max_ind = -1 # the maximum likely state from which this one came
        for prev_state in states:
          prev_delta = delta[ti - 1][prev_state]    # The likelihood of the most likely path to the previous state
          if curr_state == 0: trans_prob = p_null   # If we're drawing from NULL, we should use p_null
          else:                                     # Otherwise, step back and look for the first non-null state in the chain
            prev_prev = prev_state
            prev_time = ti - 1
            while prev_prev == 0 and prev_time > 0:
              prev_prev -= gamma[prev_time][prev_prev] # step one state back along the chain
              prev_time -= 1                           # we're going back in time
            if prev_prev > 0: trans_prob = not_p_null * s[curr_state - prev_prev] / s_denom[prev_prev]
            else: trans_prob = pi[curr_state]          # Once again, if it's all NULL up to here then we just say we started here
          prod = prev_delta * trans_prob
          if prod > max_val:
            max_val = prod
            max_ind = prev_state
        ### Heuristic: if the word appears exactly in both sentences, it should not be translated from another word
        ####################################################
        if pair[1][ti] in pair[0] and pair[0][curr_state] != pair[1][ti]: t_prob = 0.0
        else: t_prob = t[(pair[1][ti], pair[0][curr_state])]
        #####################################################
        delta[ti][curr_state] = t_prob * max_val
        gamma[ti][curr_state] = max_ind
    # start building the path, back to front
    max_val = -1
    for (i, val) in enumerate(delta[-1]):
      if val > max_val:
        max_val = val
        pair[2][-1] = i
    for i in reversed(time[:-1]):
      pair[2][i] = gamma[i + 1][pair[2][i+1]]

# for pair in pairs:
#   for (i, w1) in enumerate(pair[1]):
#     max_val = -1
#     max_index = -1
#     for (j, w0) in enumerate(pair[0]):
#       prob = t[(w1, w0)]
#       if prob > max_val:
#         max_val = prob
#         max_index = j
#     if max_index != 0: sys.stdout.write("%i-%i " % (max_index - 1, i))
#   sys.stdout.write("\n")  

for i in range(opts.hmm):
  sys.stderr.write("Now doing iteration %i for hmm\n" % (i + 1))
  viterbi()
  (p_null, s, pi) = update_params()
viterbi()  

for pair in pairs:
  for (i, a) in enumerate(pair[2]):
    if a != 0: sys.stdout.write("%i-%i " % (a - 1, i))
  sys.stdout.write("\n")  



